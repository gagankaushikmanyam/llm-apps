Author: Gagan Kaushik Manyam  
---

# üß™ LLM Lab ‚Äî A Systems-First Playground for Modern LLM Engineering

**LLM Lab** is a modular, Streamlit-based experimentation environment for learning and demonstrating  
**how real-world LLM systems are designed, debugged, and extended** ‚Äî beyond prompt demos and surface-level examples.

This repository focuses on **system behavior**, not model hype.

It explores:

‚Ä¢ supervised fine-tuning  
‚Ä¢ hallucination mitigation  
‚Ä¢ retrieval-grounded generation (RAG-lite)  
‚Ä¢ multi-step orchestration  
‚Ä¢ tool-based (MCP-style) execution  
‚Ä¢ prompt caching and latency optimization  

All examples are:

‚Ä¢ CPU-friendly  
‚Ä¢ fully inspectable  
‚Ä¢ explicit about failure modes  

This is a **learning + research lab**, not a production framework.

---

## üë§ Who This Repository Is For

This repository is designed for:

‚Ä¢ Aspiring **AI / LLM Engineers** entering industry roles  
‚Ä¢ Software / ML engineers transitioning into **LLM systems**  
‚Ä¢ Researchers who want to understand *why* LLMs fail or succeed  
‚Ä¢ Recruiters and hiring managers evaluating **system-level thinking**

If you want to understand:
‚Ä¢ why hallucinations happen  
‚Ä¢ why prompting is not enough  
‚Ä¢ how orchestration actually works  
‚Ä¢ how tools and caching change LLM behavior  

this repository is for you.

---

## ‚ú® Design Principles

‚Ä¢ Inspectability over magic  
‚Ä¢ Concept-first demos (why things work or fail)  
‚Ä¢ CPU-first, GPU optional  
‚Ä¢ Reproducibility via explicit seeds  
‚Ä¢ Plugin-style architecture  
‚Ä¢ No hidden datasets, no black boxes  

---

## üèó Architecture Overview

The lab is built around a **single Streamlit launcher**.

Core entry point:
‚Ä¢ app.py  

Applications are auto-discovered from:
‚Ä¢ applications/  

### üîå Application Interface Contract

Each application must expose:

APP_NAME ‚Äî Human-readable title  
APP_DESCRIPTION ‚Äî Optional description  
run() ‚Äî Streamlit entrypoint  

Adding a new app:
‚Ä¢ Drop a file into applications/  
‚Ä¢ Restart Streamlit  
‚Ä¢ No launcher changes required  

This keeps the system clean, scalable, and extensible.

---

## üß† Application Overview

### App 1 ‚Äî Hugging Face Fine-Tuning (Supervised)

File: applications/finetuning.py  

Demonstrates **end-to-end supervised fine-tuning** of a causal language model using Hugging Face Transformers.

What this app shows:
‚Ä¢ True before-vs-after comparison  
‚Ä¢ Validation loss with early stopping  
‚Ä¢ Holdout benchmark not seen during training  
‚Ä¢ Simple evaluation metrics  
  ‚Äì Exact Match  
  ‚Äì Token-level F1  
‚Ä¢ Saved training artifacts  

Artifacts location:
‚Ä¢ artifacts/finetuning/<timestamp>/  

Models:
‚Ä¢ sshleifer/tiny-gpt2 ‚Äî ultra-fast, educational  
‚Ä¢ distilgpt2 ‚Äî higher quality, still CPU-friendly  

Important note:
‚Ä¢ This app performs **full fine-tuning**
‚Ä¢ LoRA / QLoRA are planned extensions  

Key lesson:
Fine-tuning improves task alignment, **not knowledge**, and overfits easily on small datasets.

---

### App 2 ‚Äî Hallucinations Lab (Prompting + RAG-lite)

File: applications/hallucinations.py  

Demonstrates **why hallucinations occur** and why **grounding with retrieved context** is the only reliable mitigation strategy.

Baseline behavior:
‚Ä¢ No structure  
‚Ä¢ No refusal  
‚Ä¢ No grounding  

Result:
‚Ä¢ Fluent answers  
‚Ä¢ Confident tone  
‚Ä¢ Fabricated facts  

Prompting techniques (JSON, refusal, self-consistency):
‚Ä¢ Improve formatting  
‚Ä¢ Improve stability  
‚Ä¢ Do NOT guarantee correctness  

Key insight:
Prompting reduces chaos ‚Äî **not hallucinations**.

Context-only answering (RAG-lite):
‚Ä¢ Model may ONLY answer using retrieved context  
‚Ä¢ Must return UNKNOWN if unsupported  

Knowledge base:
‚Ä¢ Local text files under knowledge_base/  
‚Ä¢ Fully explicit and inspectable  

Retrieval:
‚Ä¢ TF-IDF via scikit-learn  
‚Ä¢ Chunking + similarity ranking  
‚Ä¢ Top-K context injection  

Key lesson:
Hallucinations are a **system design problem**, not a model bug.

---

### App 3 ‚Äî LangChain Orchestration (Multi-Step Reasoning)

File: applications/langchain_orchestration.py  

Demonstrates **explicit multi-step orchestration** using LangChain.

Pipeline stages:
1. Classification  
2. Clarifying questions  
3. Checklist and required documents  
4. Optional email draft  

Each step:
‚Ä¢ Executes independently  
‚Ä¢ Consumes prior output  
‚Ä¢ Is visible in the UI  

Key lesson:
Orchestration enables control, traceability, and debuggability ‚Äî mirroring real enterprise workflows.

---

### App 4 ‚Äî MCP Tools Lab (Tool-Based Systems)

File: applications/mcp_tax_tools.py  

Demonstrates **tool-driven LLM systems** inspired by the Model Context Protocol (MCP).

Tools implemented:
‚Ä¢ classify_tax_case  
‚Ä¢ build_prep_checklist  
‚Ä¢ draft_tax_email  

Each tool is:
‚Ä¢ Deterministic  
‚Ä¢ Typed  
‚Ä¢ Auditable  

UI shows:
‚Ä¢ Live logs  
‚Ä¢ Progress indicators  
‚Ä¢ Intermediate tool calls  
‚Ä¢ Final composed output  

Key lesson:
Tools turn LLMs from text generators into **inspectable systems**.

---

### App 5 ‚Äî Prompt Caching Lab (Latency Optimization)

File: applications/prompt_caching.py  

Demonstrates **prompt caching and KV-cache reuse** for performance optimization.

What is measured:
‚Ä¢ Latency without caching (full recomputation)  
‚Ä¢ Latency with caching (shared prefix reused)  

UI displays:
‚Ä¢ Before vs after latency  
‚Ä¢ Per-query timings  
‚Ä¢ Average speed-up  
‚Ä¢ Output comparison  

Key lesson:
Prompt caching does not change correctness ‚Äî it dramatically improves latency and scalability.

---

## üß© How Everything Fits Together

| Component          | What It Teaches                         |
|--------------------|------------------------------------------|
| Fine-tuning        | Weight adaptation                        |
| Hallucinations     | Why grounding is required                |
| Orchestration      | Structured reasoning                    |
| MCP Tools          | Controlled execution                    |
| Prompt Caching     | Performance and latency optimization    |

Together, these demonstrate **modern LLM system design**.

---

## ‚ñ∂Ô∏è Running the Lab

Steps:
1. Create virtual environment  
2. Install dependencies  
3. Launch Streamlit  

Commands:

python -m venv llms-venv  
source llms-venv/bin/activate  
python -m pip install -r requirements.txt  
python -m streamlit run app.py  

Always run Streamlit via python -m to ensure the correct environment.

---

## üöÄ Roadmap

Planned additions:
‚Ä¢ LoRA / QLoRA fine-tuning  
‚Ä¢ Full RAG with embeddings  
‚Ä¢ LangGraph workflows  
‚Ä¢ MCP protocol integrations  
‚Ä¢ Multi-agent coordination  
‚Ä¢ ML & AI classics (trees, sparse regression, neural nets)  

---

## üß† Final Takeaway

This repository is not about making LLMs sound smart.

It is about understanding:
‚Ä¢ why they fail  
‚Ä¢ how systems constrain them  
‚Ä¢ how engineers make them reliable  

That is the difference between **demos** and **production systems**.

---

‚≠ê If this repo helped you learn something ‚Äî consider starring it.  
üí¨ If you‚Äôre hiring ‚Äî this repo reflects how I think about AI systems.