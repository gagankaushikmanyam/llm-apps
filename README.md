
‚∏ª

# LLM Lab üß™

LLM Lab is a modular, Streamlit-based experimentation environment for exploring core **Large Language Model (LLM) techniques**, starting with **supervised fine-tuning** and extending toward hallucination mitigation, LoRA/QLoRA, RAG, and agent-style systems.

The repository emphasizes:
- clear, inspectable implementations
- CPU-friendly demos with optional GPU acceleration
- reproducibility (seed control)
- a plugin-style architecture for adding new apps

---

## Architecture Overview

The project is structured around a single Streamlit launcher (`app.py`) that dynamically discovers and loads applications from the `applications/` directory.

Each application must expose:

```python
APP_NAME = "Human-readable name"
APP_DESCRIPTION = "Optional description"

def run() -> None:
    ...

New experiments are added by dropping a new file into applications/‚Äîno core launcher modifications are required.

‚∏ª

App: Hugging Face Fine-tuning Demo

File: applications/finetuning.py

This app demonstrates end-to-end supervised fine-tuning of a causal language model using the Hugging Face Transformers ecosystem, with a direct comparison between pretrained and fine-tuned behavior.

Task

Logistics email subject line generation from short instructions.

Data

A small in-repo toy dataset defined in utils/io.py (instruction ‚Üí subject).

Model
	‚Ä¢	Default: sshleifer/tiny-gpt2 (CPU-friendly)
	‚Ä¢	Optional: distilgpt2 (higher quality, slower on CPU)

Training + Evaluation
	‚Ä¢	Examples are formatted as:
Instruction: ...\nSubject: ...
	‚Ä¢	Tokenization and training use the standard causal LM objective (labels = input_ids)
	‚Ä¢	Training runs via Hugging Face Trainer
	‚Ä¢	Loss per epoch is displayed
	‚Ä¢	Fine-tuned artifacts are saved under:
artifacts/finetuning/<timestamp>/

Expected outcome
	‚Ä¢	‚ÄúAfter‚Äù output becomes more task-aligned than ‚ÄúBefore‚Äù
	‚Ä¢	With very small datasets, excessive epochs can cause repetition (overfitting),
mitigated via decoding constraints (greedy/beam + repetition controls)

‚∏ª

App: Hallucinations Lab ‚Äî Prompting + RAG-lite Grounding

File: applications/hallucinations.py

This app demonstrates why hallucinations happen, why prompting alone is insufficient, and how grounding with retrieved context (RAG-lite) is the only reliable way to reduce hallucinations in practice.

The goal is not to make a small model ‚Äúknow facts‚Äù, but to show how systems enforce correctness even when the model is unreliable.

‚∏ª

Why Hallucinations Happen (Baseline)

Large Language Models are probabilistic text generators, not truth engines.

When you ask a factual question without constraints, the model will:
	‚Ä¢	Produce a fluent answer
	‚Ä¢	Sound confident
	‚Ä¢	Hallucinate if it does not know

Baseline Mode (Free-form)

Technique:
	‚Ä¢	No structure
	‚Ä¢	No refusal
	‚Ä¢	No grounding

Expected behavior:
	‚Ä¢	The model always answers
	‚Ä¢	Often confidently wrong
	‚Ä¢	No way to verify correctness

How to test it:
	1.	Select Technique ‚Üí Baseline (free-form)
	2.	Ask a nonsense or unknown question:
	‚Ä¢	‚ÄúWhat year did Isaac Newton invent the smartphone?‚Äù
	3.	Observe:
	‚Ä¢	The model gives a confident but fabricated answer

This demonstrates the default hallucination behavior of LLMs.

‚∏ª

Why Prompting Alone Is Not Enough

JSON-only / Refusal / Self-consistency Modes

These techniques improve output control, not truth.

They help with:
	‚Ä¢	Structured outputs
	‚Ä¢	Safer responses (UNKNOWN)
	‚Ä¢	Stability across multiple generations

They do not guarantee correctness unless the model already knows the answer.

How to test:
	‚Ä¢	Use JSON-only or JSON + refusal
	‚Ä¢	Ask factual questions the model may or may not know
	‚Ä¢	You may still get:
	‚Ä¢	Wrong answers
	‚Ä¢	Or inconsistent answers across runs

This shows:

Prompting reduces chaos, not hallucinations.

‚∏ª

Context-Only Answering (Grounded Mode)

This is the core hallucination solution demonstrated in this app.

What ‚ÄúContext-Only‚Äù Actually Means
	‚Ä¢	The model is forbidden from using its internal knowledge
	‚Ä¢	It may only answer using retrieved text
	‚Ä¢	If the answer is not supported ‚Üí it must return UNKNOWN

This is a RAG-lite system.

‚∏ª

Knowledge Base (Local, Explicit, Transparent)

You must create a local knowledge base manually.

Folder structure:

knowledge_base/
  australia.txt
  logistics_faq.txt
  ...

Example (australia.txt):

Australia's national government is based in Canberra, home to Parliament House.
Sydney is the largest city by population.

There is no hidden dataset and no magic.

This is intentional:
	‚Ä¢	You control the knowledge
	‚Ä¢	You can inspect exactly what the model sees
	‚Ä¢	You can test failure cases honestly

‚∏ª

How Retrieval Works (RAG-lite)
	1.	Documents are split into chunks
	2.	TF-IDF (scikit-learn) ranks chunks by similarity to the question
	3.	Top-K chunks are retrieved
	4.	The model is only allowed to answer using those chunks

This is why scikit-learn is installed:
	‚Ä¢	It powers local retrieval
	‚Ä¢	No embeddings, no vector DB, no cloud
	‚Ä¢	Simple, transparent, CPU-friendly

‚∏ª

How to Test Context-Only Correctness

Correct Answer Case
	1.	Technique ‚Üí Context-only (RAG-lite grounded)
	2.	Question:

What is the capital of Australia?


	3.	Ensure australia.txt contains the answer
	4.	Expected output:
	‚Ä¢	answer: Canberra
	‚Ä¢	supported_by_context: true
	‚Ä¢	Evidence quoted from the document

Forced UNKNOWN Case
	1.	Ask:

Who is the president of Australia?


	2.	If the answer is not in the documents
	3.	Expected output:
	‚Ä¢	answer: UNKNOWN
	‚Ä¢	supported_by_context: false

This verifies that hallucination is blocked, not hidden.

‚∏ª

Why Context-Only May Feel ‚ÄúObvious‚Äù

You may notice:

‚ÄúWe already put the answer in the context.‚Äù

That is the entire point.

In real systems:
	‚Ä¢	Context comes from databases
	‚Ä¢	Documents
	‚Ä¢	APIs
	‚Ä¢	Logs
	‚Ä¢	Contracts
	‚Ä¢	Internal knowledge bases

The model‚Äôs job is not to invent, but to:
	‚Ä¢	Read
	‚Ä¢	Extract
	‚Ä¢	Cite
	‚Ä¢	Refuse when unsupported

This app demonstrates that principle clearly.

‚∏ª

Summary: What Each Mode Teaches You

Mode	What it demonstrates
Baseline	Confident hallucinations
JSON-only	Structured but not factual
Refusal	Safer uncertainty
Self-consistency	Stability, not truth
Context-only (RAG-lite)	Actual hallucination prevention


‚∏ª

Key Takeaway

Hallucinations are not a ‚Äúmodel bug‚Äù.
They are a system design problem.

This app shows that:
	‚Ä¢	Prompting helps formatting
	‚Ä¢	Retrieval provides truth
	‚Ä¢	Grounding enforces correctness

Once this is clear, extending to:
	‚Ä¢	Full RAG
	‚Ä¢	Vector databases
	‚Ä¢	Citations
	‚Ä¢	Tools & agents
becomes straightforward.

‚∏ª

Running the Lab

python -m venv llms-venv
source llms-venv/bin/activate
python -m pip install -r requirements.txt
python -m streamlit run app.py

Always use python -m streamlit to ensure Streamlit runs inside the correct environment.

‚∏ª

Extending the Lab

To add a new application:
	1.	Create applications/<new_app>.py
	2.	Define APP_NAME and run()
	3.	Restart Streamlit

Suggested next apps:
	‚Ä¢	applications/lora.py
	‚Ä¢	applications/qlora.py
	‚Ä¢	applications/rag.py
	‚Ä¢	applications/mcp.py