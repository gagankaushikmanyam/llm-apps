
â¸»


# LLM Lab ðŸ§ª

LLM Lab is a modular, Streamlit-based experimentation environment for exploring and demonstrating core **Large Language Model (LLM) techniques**, starting with **supervised fine-tuning** and designed to scale toward LoRA, QLoRA, RAG, and agent-based systems.

The repository prioritizes:
- clarity of implementation
- reproducibility
- CPU-friendly experimentation
- clean extensibility through a plugin-style app architecture

---

## Architecture Overview

The project is structured around a **single Streamlit launcher** (`app.py`) that dynamically discovers and loads LLM mini-applications from the `applications/` directory.

### Application discovery
- `app.py` scans all Python files in `applications/`
- Each file represents an independent LLM experiment
- No manual registration is required

Each application must expose the following interface:

```python
APP_NAME = "Human-readable application name"

def run() -> None:
    ...

Discovered applications are rendered automatically in the Streamlit sidebar, and their UI is displayed in the main panel upon selection.

This design enables seamless extension of the lab by adding new application files without modifying core infrastructure.

â¸»

Included Application

App 1: Hugging Face Fine-tuning Demo

File: applications/finetuning.py

This application demonstrates end-to-end supervised fine-tuning of a causal language model using the Hugging Face Transformers ecosystem, with a clear comparison between pretrained and fine-tuned model behavior.

â¸»

Background: What is Fine-tuning?

Pretrained language models (e.g., GPT-2) are trained on large, general-purpose corpora.
They exhibit strong linguistic competence but lack specialization for specific downstream tasks.

Fine-tuning refers to continuing training on a smaller, task-specific dataset in order to adapt the modelâ€™s internal weights to a particular domain or output style.

Key characteristics:
	â€¢	updates model parameters (weights)
	â€¢	differs fundamentally from prompt engineering
	â€¢	enables task specialization with limited data

â¸»

Fine-tuning in This Repository (Toy Example)

Task Definition

The fine-tuning task implemented here is logistics email subject line generation.

Given a short instruction describing an operational scenario, the model is trained to generate a concise, professional email subject line.

Example input

Write an email subject for a shipment delayed due to weather.
Mention the new ETA is tomorrow.

Expected output

Weather Delay: Updated ETA for Shipment (Arrives Tomorrow)


â¸»

Implementation Details

Data
	â€¢	Dataset is defined in utils/io.py
	â€¢	Each training example consists of:
	â€¢	instruction: textual description of the scenario
	â€¢	subject: target email subject line
	â€¢	The dataset is intentionally small to ensure:
	â€¢	fast execution on CPU
	â€¢	clear visibility of training effects

This dataset is designed for demonstration and learning, not for production-grade performance.

â¸»

Model
	â€¢	Default model: sshleifer/tiny-gpt2
	â€¢	extremely lightweight
	â€¢	CPU-friendly
	â€¢	suitable for rapid experimentation
	â€¢	Optional alternative: distilgpt2
	â€¢	higher capacity
	â€¢	improved output quality
	â€¢	slower on CPU

Models are loaded via Hugging Face Transformers.

â¸»

Training Procedure
	1.	Each example is formatted as a single causal language modeling sequence:

Instruction: <instruction text>
Subject: <subject text>

	2.	Text is tokenized and converted into model inputs
	3.	Labels are set equal to input IDs (standard causal LM objective)
	4.	Training is performed using Hugging Faceâ€™s Trainer API
	5.	Training runs for a small number of epochs to avoid overfitting
	6.	The fine-tuned model and tokenizer are saved locally under:

artifacts/finetuning/<timestamp>/


â¸»

Evaluation and Comparison

The application performs side-by-side inference using the same instruction:
	â€¢	once with the base pretrained model
	â€¢	once with the fine-tuned model

This direct comparison highlights how fine-tuning alters model behavior for the target task.

Training loss per epoch is plotted to provide visibility into optimization dynamics.

â¸»

Generation Strategy

Because the dataset is intentionally small, the generation pipeline applies stabilizing constraints to reduce repetition and overfitting artifacts:
	â€¢	greedy or beam decoding (configurable)
	â€¢	repetition penalty
	â€¢	no-repeat n-gram constraints

These choices prioritize interpretability and consistency over creative diversity.

â¸»

Expected Outcome

After fine-tuning:
	â€¢	outputs become more structured and task-aligned
	â€¢	subject lines exhibit clearer logistics-oriented phrasing
	â€¢	differences between pretrained and fine-tuned behavior are immediately observable

It is expectedâ€”and instructiveâ€”that excessive epochs on small datasets can lead to repetition, illustrating common fine-tuning failure modes.

â¸»

Running the Application

python -m venv llms-venv
source llms-venv/bin/activate
python -m pip install -r requirements.txt
python -m streamlit run app.py

Always use python -m streamlit to ensure the correct virtual environment is used.

â¸»

Extensibility

The repository is designed to grow incrementally.

New experiments (e.g., LoRA, QLoRA, RAG, MCP tools) can be added by:
	1.	creating a new file in applications/
	2.	defining APP_NAME and run()
	3.	restarting the Streamlit app

No changes to the launcher are required.

â¸»

Scope and Intent

LLM Lab is focused on mechanistic understanding and experimentation, not on maximizing text quality.

The goal is to provide a clean, inspectable foundation for:
	â€¢	understanding how fine-tuning works in practice
	â€¢	observing common training behaviors and failure modes
	â€¢	extending toward more advanced LLM adaptation techniques

This foundation makes subsequent work on parameter-efficient fine-tuning, retrieval augmentation, and agent systems significantly easier to reason about.

---
