
‚∏ª

# LLM Lab üß™

LLM Lab is a modular, Streamlit-based experimentation environment for exploring core **Large Language Model (LLM) techniques**, starting with **supervised fine-tuning** and extending toward hallucination mitigation, LoRA/QLoRA, RAG, and agent-style systems.

The repository emphasizes:
- clear, inspectable implementations
- CPU-friendly demos with optional GPU acceleration
- reproducibility (seed control)
- a plugin-style architecture for adding new apps

---

## Architecture Overview

The project is structured around a single Streamlit launcher (`app.py`) that dynamically discovers and loads applications from the `applications/` directory.

Each application must expose:

```python
APP_NAME = "Human-readable name"
APP_DESCRIPTION = "Optional description"

def run() -> None:
    ...

New experiments are added by dropping a new file into applications/‚Äîno core launcher modifications are required.

‚∏ª

App 1: Hugging Face Fine-tuning Demo

File: applications/finetuning.py

This app demonstrates end-to-end supervised fine-tuning of a causal language model using the Hugging Face Transformers ecosystem, with a direct comparison between pretrained and fine-tuned behavior.

Task

Logistics email subject line generation from short instructions.

Data

A small in-repo toy dataset defined in utils/io.py (instruction ‚Üí subject).

Model
	‚Ä¢	Default: sshleifer/tiny-gpt2 (CPU-friendly)
	‚Ä¢	Optional: distilgpt2 (higher quality, slower on CPU)

Training + Evaluation
	‚Ä¢	Examples are formatted as:
Instruction: ...\nSubject: ...
	‚Ä¢	Tokenization and training use the standard causal LM objective (labels = input_ids)
	‚Ä¢	Training runs via Hugging Face Trainer
	‚Ä¢	Loss per epoch is displayed
	‚Ä¢	Fine-tuned artifacts are saved under:
artifacts/finetuning/<timestamp>/

Expected outcome
	‚Ä¢	‚ÄúAfter‚Äù output becomes more task-aligned than ‚ÄúBefore‚Äù
	‚Ä¢	With very small datasets, excessive epochs can cause repetition (overfitting),
mitigated via decoding constraints (greedy/beam + repetition controls)

‚∏ª

App 2: Hallucinations Lab (Prompting Techniques)

File: applications/hallucinations.py

This app demonstrates prompt-level techniques to reduce hallucinations by improving output controllability and encouraging uncertainty. These approaches do not guarantee factual correctness without grounding (retrieval, citations, tools), but they are useful building blocks.

Techniques included
	1.	Baseline (free-form): unconstrained responses can sound confident even when wrong
	2.	JSON-only format: forces structured output and improves parseability
	3.	JSON + refusal policy: permits explicit uncertainty via UNKNOWN + confidence
	4.	Context-only answering: model must answer using provided context, else UNKNOWN
	5.	Self-consistency voting: sample multiple JSON answers and pick the most frequent

Expected outcome
	‚Ä¢	JSON prompting tends to reduce rambling and makes outputs machine-checkable
	‚Ä¢	Refusal policies reduce hallucinations when the model is uncertain
	‚Ä¢	Context-only prompts emulate a minimal ‚Äúgrounded answering‚Äù rule
	‚Ä¢	Self-consistency improves stability when single generations are noisy

‚∏ª

Running the Lab

python -m venv llms-venv
source llms-venv/bin/activate
python -m pip install -r requirements.txt
python -m streamlit run app.py

Always use python -m streamlit to ensure Streamlit runs inside the correct environment.

‚∏ª

Extending the Lab

To add a new application:
	1.	Create applications/<new_app>.py
	2.	Define APP_NAME and run()
	3.	Restart Streamlit

Suggested next apps:
	‚Ä¢	applications/lora.py
	‚Ä¢	applications/qlora.py
	‚Ä¢	applications/rag.py
	‚Ä¢	applications/mcp.py