Author: Gagan Kaushik Manyam  
---

# ğŸ§ª LLM Lab â€” A Systems-First Playground for Practical LLM Engineering

**LLM Lab** is a modular, Streamlit-based environment to **learn, test, and debug real LLM system behaviors**â€”from fine-tuning and hallucination mitigation to orchestration, tool usage, and full Retrieval-Augmented Generation (RAG).

This repo is **concept-first** and **system-first**:
- It shows **why** outputs fail (hallucinations, weak retrieval, overfitting),
- and **how** engineering patterns (grounding, orchestration, tools, re-ranking, strict outputs) make them reliable.

All demos are designed to be:
- **CPU-friendly by default** (GPU optional),
- **inspectable** (no hidden magic),
- and **reproducible** (seed controls, deterministic knobs).

> This is a learning + research lab, not a production framework.

---

## ğŸ‘¤ Who This Repository Is For

This repository is built for:
- **Aspiring AI / LLM Engineers** starting industry roles
- **Software / ML Engineers** transitioning into LLM systems
- **Researchers** who want clarity on *why systems fail/succeed*
- **Recruiters / hiring managers** evaluating practical systems skills

If you want to understand:
- why LLMs hallucinate,
- why â€œprompting harderâ€ isnâ€™t enough,
- how retrieval and citations enforce correctness,
- how orchestration and tools create reliable pipelines,

this repo is for you.

---

## âœ¨ Design Principles

- ğŸ” **Inspectability over magic**
- ğŸ§  **Concept-first demos** (failure modes are part of the learning)
- ğŸ’» **CPU-first**, GPU optional
- ğŸ¯ **Reproducibility** (explicit seeds + deterministic modes)
- ğŸ§© **Plugin-style architecture** (drop-in apps)
- ğŸš« No hidden datasets, no black boxes

---

## ğŸ— Architecture Overview

The lab is driven by one Streamlit launcher:

- **`app.py`** â€” discovers and loads all apps under **`applications/`**

Every app must expose:

- `APP_NAME` (required)
- `APP_DESCRIPTION` (optional)
- `run() -> None` (required)

Add a new app by creating:

- `applications/<new_app>.py`

â€¦and restarting Streamlit.

---

## ğŸ“Œ Quick Summary of All Apps

| App | File | What it teaches | Primary â€œSystem Skillâ€ |
|---|---|---|---|
| Fine-tuning | `applications/finetuning.py` | Adapting model weights to a task | Training + evaluation discipline |
| Hallucinations Lab | `applications/hallucinations.py` | Why hallucinations happen + how to block them | Grounding + refusal + verification |
| LangChain Orchestration | `applications/langchain_orchestration.py` | Multi-step pipelines with traceable steps | Orchestration + debuggability |
| MCP Tax Tools | `applications/mcp_tax_tools.py` | Tool-based execution with logging | Deterministic, auditable actions |
| Full RAG (Chroma) | `applications/full_rag_chroma.py` | Retrieval + re-ranking + strict citations | Retrieval quality + evidence-first answers |

---

## â–¶ï¸ Running the Lab

```bash
python -m venv llms-venv
source llms-venv/bin/activate
python -m pip install -r requirements.txt
python -m streamlit run app.py

Important: Prefer python -m streamlit to ensure Streamlit runs inside the active venv.

â¸»

ğŸ§  App 1 â€” Hugging Face Fine-Tuning (Supervised)

ğŸ“„ File: applications/finetuning.py

What it is

Supervised fine-tuning continues training a pretrained model on a small task dataset to shift its behavior toward your domain.

Mathematically, you minimize cross-entropy over tokens:
[
\mathcal{L} = -\sum_{t} \log p_\theta(x_t \mid x_{<t})
]
Youâ€™re updating weights (\theta), not just changing the prompt.

Intended goal

Teach a model to generate logistics email subject lines from instructions.

Advantages
	â€¢	âœ… Improves task formatting and domain style (tone, structure, key terms)
	â€¢	âœ… Demonstrates training dynamics (overfitting vs generalization)
	â€¢	âœ… Shows the difference between â€œmodel behaviorâ€ vs â€œprompt tricksâ€

What this app shows
	â€¢	TRUE Before vs After (fresh base model vs fine-tuned model)
	â€¢	Train & validation loss curves
	â€¢	Early stopping (prevents overfitting/repetition)
	â€¢	Holdout benchmark (examples not seen during training)
	â€¢	Simple metrics:
	â€¢	Exact Match
	â€¢	Token-level F1
	â€¢	Artifacts saved under:
	â€¢	artifacts/finetuning/<timestamp>/

Example

Instruction
	â€¢	Write an email subject for a shipment delayed due to weather. Mention the new ETA is tomorrow.

Expected outcome after fine-tuning
	â€¢	â€œWeather Delay: Updated ETA â€” Delivery Tomorrowâ€

Note: This app performs full fine-tuning, not LoRA/QLoRA.

â¸»

ğŸ§  App 2 â€” Hallucinations Lab (Prompting + Grounding)

ğŸ“„ File: applications/hallucinations.py

What it is

Hallucinations happen because LLMs are probabilistic next-token predictors, not truth engines.
Without grounding, the model tries to produce a plausible continuation even when it lacks facts.

Intended goal

Show:
	1.	how hallucinations appear in baseline prompting
	2.	why formatting constraints help structure but not truth
	3.	why grounding (context-only) blocks hallucinations
	4.	how a â€œRAG-liteâ€ pattern improves reliability

Advantages
	â€¢	âœ… Makes hallucination behavior visible and testable
	â€¢	âœ… Teaches refusal behavior (UNKNOWN) as a safety mechanism
	â€¢	âœ… Demonstrates grounding rules (â€œonly answer if supportedâ€)

Modes included (what to learn)
	â€¢	Baseline (free-form): fluent + confident + wrong is common
	â€¢	JSON-only: better structure, still wrong if model lacks knowledge
	â€¢	Refusal policy: allows the model to say UNKNOWN
	â€¢	Self-consistency: improves stability, not factuality
	â€¢	Context-only: enforces truth by limiting allowed information

Example tests

Baseline hallucination test

Ask:
	â€¢	â€œWhat year did Isaac Newton invent the smartphone?â€

Expected:
	â€¢	A confident fabricated answer (hallucination).

Context-only correctness test

Put into context:
	â€¢	â€œAustraliaâ€™s capital city is Canberra.â€

Ask:
	â€¢	â€œWhat is the capital of Australia?â€

Expected:
	â€¢	JSON answer supported by context + evidence.

â¸»

ğŸ§  App 3 â€” LangChain Orchestration (Multi-Step Pipeline)

ğŸ“„ File: applications/langchain_orchestration.py

What it is

Orchestration decomposes a task into explicit steps. Each step has:
	â€¢	a purpose,
	â€¢	inputs,
	â€¢	outputs,
	â€¢	and can be debugged independently.

This mirrors how enterprise systems avoid â€œone huge prompt that does everythingâ€.

Intended goal

Create a tax-prep assistant pipeline that breaks work into steps:
	1.	classify the case
	2.	generate clarifying questions
	3.	produce checklist + required documents
	4.	optionally draft a structured email

Advantages
	â€¢	âœ… Traceability: you can see what each step produced
	â€¢	âœ… Debuggability: identify which step caused failure
	â€¢	âœ… Control: enforce constraints per-step (JSON, refusal, etc.)

Example

Input:
	â€¢	â€œIâ€™m filing taxes in Germany; I need a checklist and what to clarify with a tax advisor.â€

Expected:
	â€¢	Classification (category)
	â€¢	3â€“7 clarifying questions
	â€¢	Checklist of documents
	â€¢	Optional email draft to advisor

â¸»

ğŸ§  App 4 â€” MCP Tax Tools (Tool-Based Execution)

ğŸ“„ File: applications/mcp_tax_tools.py

What it is

Tool-based systems turn LLM workflows into auditable function calls:
	â€¢	each tool has typed inputs/outputs,
	â€¢	deterministic logic,
	â€¢	and logs of what happened.

This resembles the core idea behind tool protocols (MCP-style patterns).

Intended goal

Demonstrate how an assistant can call tools like:
	1.	classify_tax_case
	2.	build_prep_checklist
	3.	draft_tax_email

Advantages
	â€¢	âœ… Deterministic outputs for key steps
	â€¢	âœ… Auditable logs + intermediate states
	â€¢	âœ… Reduced hallucination by limiting â€œfree-form inventionâ€

Example

Input:
	â€¢	â€œI need checklist + questions before filing.â€

Expected:
	â€¢	tool call logs shown in UI
	â€¢	checklist + questions generated as structured outputs
	â€¢	email draft assembled using tool outputs

â¸»

ğŸ§  App 5 â€” Full RAG (ChromaDB + HF) with Re-Rank + Strict Citations

ğŸ“„ File: applications/full_rag_chroma.py

What it is

Retrieval-Augmented Generation (RAG) is a system design pattern:
	â€¢	retrieve relevant text from a knowledge base,
	â€¢	inject it into the prompt,
	â€¢	and force the model to answer from evidence.

Core idea (mathematically)

We want:
[
p(y \mid x) \approx \sum_{d \in \mathcal{D}} p(y \mid x, d),p(d \mid x)
]
Where:
	â€¢	(x) = question
	â€¢	(d) = retrieved document chunk
	â€¢	(p(d \mid x)) = retriever relevance score
	â€¢	(p(y \mid x, d)) = generator conditioned on retrieved evidence

Intended goal

Build a local, inspectable full RAG pipeline:
	1.	Chunk documents
	2.	Embed chunks
	3.	Store/query in ChromaDB
	4.	Retrieve Top-N candidates
	5.	Re-rank candidates for better evidence
	6.	Generate an answer with strict JSON citations + quoted evidence

Why re-ranking is added

Vector retrieval is fast but approximate. It can return â€œkind of relatedâ€ chunks.

Re-ranking uses a stronger model that scores:
	â€¢	(question, chunk)

This improves retrieval quality, especially when multiple candidates look similar.

Strict answer + citations JSON (why it matters)

This app enforces a JSON contract:
	â€¢	answer
	â€¢	supported_by_context
	â€¢	citations (source + chunk id)
	â€¢	quoted_evidence (short direct quotes)

If evidence is insufficient, the model must answer:
	â€¢	UNKNOWN

This turns RAG into an auditable system instead of â€œtrust me broâ€.

Advantages
	â€¢	âœ… Retrieval-based correctness (when KB is correct)
	â€¢	âœ… Evidence-first answers
	â€¢	âœ… Stronger retrieval quality via re-rank
	â€¢	âœ… Debuggable: you can inspect retrieved chunks and scores

Example

Knowledge base includes australia.txt:
	â€¢	â€œAustraliaâ€™s national government is based in Canberraâ€¦â€

Ask:
	â€¢	â€œWhat is the capital of Australia?â€

Expected:
	â€¢	JSON answer: Canberra
	â€¢	citations show australia.txt chunk id
	â€¢	quoted evidence contains the supporting line

â¸»

ğŸš€ Roadmap

Planned additions:
	â€¢	LoRA / QLoRA fine-tuning
	â€¢	Embedding-based RAG variants + vector DB comparisons
	â€¢	LangGraph workflows
	â€¢	MCP protocol integrations
	â€¢	Multi-agent coordination

â¸»

ğŸ§  Portfolio Note (Recruiter-Friendly)

This repo demonstrates practical LLM systems skills:
	â€¢	training discipline (eval, overfitting control)
	â€¢	hallucination mitigation via grounding
	â€¢	retrieval + re-ranking + citations
	â€¢	orchestration (step-by-step pipelines)
	â€¢	tool-based execution and logging

If youâ€™re hiring for AI/LLM roles, this repo reflects how I design systems:
inspectable, auditable, and resilient.

â­ If this repo helped you learn something â€” consider starring it.
ğŸ’¬ If youâ€™re hiring â€” feel free to reach out.

